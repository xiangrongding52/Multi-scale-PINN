{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37015511-bdbf-4e50-a769-99b8d0011b95",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-02T15:06:15.835920Z",
     "iopub.status.busy": "2025-03-02T15:06:15.835582Z",
     "iopub.status.idle": "2025-03-02T15:16:24.803776Z",
     "shell.execute_reply": "2025-03-02T15:16:24.802759Z",
     "shell.execute_reply.started": "2025-03-02T15:06:15.835897Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 500/500 [00:09<00:00, 54.75it/s, Iter=500, Loss=1.02e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam done!\n",
      "Iter: 1000 Loss: 0.004372057039290667\n",
      "Iter: 2000 Loss: 0.0009662352968007326\n",
      "LBGFS done!\n",
      "change_counts 0 Test_L2error: 1.51e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 500/500 [00:08<00:00, 58.49it/s, Iter=3000, Loss=2.28e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam done!\n",
      "Iter: 4000 Loss: 0.001278355484828353\n",
      "LBGFS done!\n",
      "change_counts 1 Test_L2error: 1.15e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 500/500 [00:08<00:00, 59.08it/s, Iter=5352, Loss=6.16e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam done!\n",
      "Iter: 6000 Loss: 0.0009544722852297127\n",
      "LBGFS done!\n",
      "change_counts 2 Test_L2error: 1.04e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 500/500 [00:08<00:00, 58.60it/s, Iter=6826, Loss=1.44e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam done!\n",
      "Iter: 7000 Loss: 0.006263567600399256\n",
      "Iter: 8000 Loss: 0.0010499669006094337\n",
      "LBGFS done!\n",
      "change_counts 3 Test_L2error: 1.46e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 500/500 [00:08<00:00, 58.81it/s, Iter=9320, Loss=3.36e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam done!\n",
      "Iter: 10000 Loss: 0.002876031445339322\n",
      "Iter: 11000 Loss: 0.0008618690771982074\n",
      "Iter: 12000 Loss: 0.0005101886345073581\n",
      "LBGFS done!\n",
      "change_counts 4 Test_L2error: 1.20e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 500/500 [00:08<00:00, 59.01it/s, Iter=13379, Loss=2.85e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam done!\n",
      "Iter: 14000 Loss: 0.0026119316462427378\n",
      "Iter: 15000 Loss: 0.0008893940248526633\n",
      "LBGFS done!\n",
      "change_counts 5 Test_L2error: 9.75e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 500/500 [00:08<00:00, 58.81it/s, Iter=16278, Loss=6.98e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam done!\n",
      "Iter: 17000 Loss: 0.0012634295271709561\n",
      "LBGFS done!\n",
      "change_counts 6 Test_L2error: 1.45e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 500/500 [00:08<00:00, 58.83it/s, Iter=18149, Loss=2.08e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam done!\n",
      "Iter: 19000 Loss: 0.001760466257110238\n",
      "Iter: 20000 Loss: 0.0009109278325922787\n",
      "LBGFS done!\n",
      "change_counts 7 Test_L2error: 1.71e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 500/500 [00:08<00:00, 58.99it/s, Iter=20624, Loss=1.33e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam done!\n",
      "Iter: 21000 Loss: 0.003934767097234726\n",
      "Iter: 22000 Loss: 0.001355780172161758\n",
      "Iter: 23000 Loss: 0.0007496742764487863\n",
      "LBGFS done!\n",
      "change_counts 8 Test_L2error: 1.08e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 500/500 [00:08<00:00, 58.71it/s, Iter=23661, Loss=5.04e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam done!\n",
      "Iter: 24000 Loss: 0.005387610290199518\n",
      "LBGFS done!\n",
      "change_counts 9 Test_L2error: 5.51e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 500/500 [00:08<00:00, 58.94it/s, Iter=24645, Loss=1.00e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam done!\n",
      "Iter: 25000 Loss: 0.0036088768392801285\n",
      "Iter: 26000 Loss: 0.0011075368383899331\n",
      "Iter: 27000 Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 641\u001b[0m\n\u001b[1;32m    629\u001b[0m u_bc \u001b[38;5;241m=\u001b[39m is_cuda(torch\u001b[38;5;241m.\u001b[39mcat((x_boundary_left_label, x_boundary_right_label), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    631\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[1;32m    632\u001b[0m     net\u001b[38;5;241m=\u001b[39mnet,\n\u001b[1;32m    633\u001b[0m     x_bc\u001b[38;5;241m=\u001b[39mx_bc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m     x_test_exact\u001b[38;5;241m=\u001b[39mx_test_exact,\n\u001b[1;32m    640\u001b[0m )\n\u001b[0;32m--> 641\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mx_test_estimate_collect)\n\u001b[1;32m    644\u001b[0m draw_exact()\n",
      "Cell \u001b[0;32mIn[1], line 372\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_baseline()\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_AM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_AM_AW1()\n",
      "Cell \u001b[0;32mIn[1], line 262\u001b[0m, in \u001b[0;36mModel.run_AM\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam done!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# if move_count == AM_count-1:\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m#     self.optimizer_LBGFS.step(self.LBGFS_epoch_loss)\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m#     print('LBGFS done!')\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_LBGFS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLBGFS_epoch_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLBGFS done!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    265\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/lbfgs.py:437\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m!=\u001b[39m max_iter:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 437\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    438\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    439\u001b[0m     opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 200\u001b[0m, in \u001b[0;36mModel.LBGFS_epoch_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# if self.start_loss_collect:\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m#     self.x_f_loss_collect.append([self.net.iter, loss_equation.item()])\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m#     self.x_label_loss_collect.append([self.net.iter, loss_label.item()])\u001b[39;00m\n\u001b[1;32m    199\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_loss(loss_equation, loss_ic, loss_bc, loss_u0t)\n\u001b[0;32m--> 200\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt, gridspec\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, trange\n",
    "from pyDOE import lhs\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "seed = 1234\n",
    "torch.set_default_dtype(torch.float)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device('cuda')\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('GPU:', use_gpu)\n",
    "\n",
    "\n",
    "def random_fun(num):\n",
    "    temp = torch.from_numpy(lb + (ub - lb) * lhs(2, num)).float()\n",
    "    if use_gpu:\n",
    "        temp = temp.cuda()\n",
    "    return temp\n",
    "\n",
    "\n",
    "def is_cuda(data):\n",
    "    if use_gpu:\n",
    "        data = data.cuda()\n",
    "    return data\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.indim = layers[0]\n",
    "        self.outdim = layers[-1]\n",
    "        self.hidden_units = layers[1:-1]\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        self.iter = 0\n",
    "        # if isinstance(hidden_units, int):\n",
    "        #     hidden_units = [hidden_units]\n",
    "        # print(f\"hidden_units inside Dense_ScaleNet: {hidden_units}, type: {type(hidden_units)}\")\n",
    "\n",
    "        input_layer = nn.Linear(self.indim, self.hidden_units[0])\n",
    "        nn.init.xavier_normal_(input_layer.weight)\n",
    "        nn.init.uniform_(input_layer.bias, -1, 1)\n",
    "        self.dense_layers.append(input_layer)\n",
    "\n",
    "        for i_layer in range(len(self.hidden_units) - 1):\n",
    "            if i_layer == 0:\n",
    "                hidden_layer = nn.Linear(2 * self.hidden_units[i_layer], self.hidden_units[i_layer + 1])\n",
    "                nn.init.xavier_normal_(hidden_layer.weight)\n",
    "                nn.init.uniform_(hidden_layer.bias, -1, 1)\n",
    "            else:\n",
    "                hidden_layer = nn.Linear(self.hidden_units[i_layer], self.hidden_units[i_layer + 1])\n",
    "                nn.init.xavier_normal_(hidden_layer.weight)\n",
    "                nn.init.uniform_(hidden_layer.bias, -1, 1)\n",
    "            self.dense_layers.append(hidden_layer)\n",
    "\n",
    "        out_layer = nn.Linear(self.hidden_units[-1], self.outdim)\n",
    "        nn.init.xavier_normal_(out_layer.weight)\n",
    "        nn.init.uniform_(out_layer.bias, -1, 1)\n",
    "        self.dense_layers.append(out_layer)\n",
    "\n",
    "    def forward(self, inputs, sFourier=0.5):\n",
    "        # ------ dealing with the input data ---------------\n",
    "        scale= [30,31,32,33,34,35,36,37,38,39,40,41,42,42,43,44,45,46,46,47,48,49]\n",
    "        #scale= np.arange(1, 31)\n",
    "        #scale = [1, 2, 4, 6, 8]\n",
    "\n",
    "        dense_in = self.dense_layers[0]\n",
    "        H = dense_in(inputs)\n",
    "\n",
    "        Unit_num = int(self.hidden_units[0] / len(scale))\n",
    "        mixcoe = np.repeat(scale, Unit_num)\n",
    "        mixcoe = np.concatenate((mixcoe, np.ones([self.hidden_units[0] - Unit_num * len(scale)]) * scale[-1]))\n",
    "        mixcoe = mixcoe.astype(np.float32)\n",
    "        torch_mixcoe = torch.from_numpy(mixcoe)\n",
    "        torch_mixcoe = torch_mixcoe.to(device)\n",
    "        H = sFourier * torch.cat([torch.cos(H * torch_mixcoe), torch.sin(H * torch_mixcoe)], dim=-1)\n",
    "\n",
    "        #  ---resnet(one-step skip connection for two consecutive layers if have equal neurons）---\n",
    "        # hiddens_record = self.hidden_units[0]\n",
    "        for k in range(len(self.hidden_units) - 1):\n",
    "            # H_pre = H\n",
    "            dense_layer = self.dense_layers[k + 1]\n",
    "            H = dense_layer(H)\n",
    "            H = torch.tanh(H)\n",
    "            # if (self.hidden_units[k+1] == hiddens_record) and (k != 0):\n",
    "            #     H = H + H_pre\n",
    "            # hiddens_record = self.hidden_units[k+1]\n",
    "\n",
    "        dense_out = self.dense_layers[-1]\n",
    "        H = dense_out(H)\n",
    "        out_results = H\n",
    "        return out_results\n",
    "\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, net, x_bc,\n",
    "                 u_bc,\n",
    "                 x_ic,\n",
    "                 u_ic, x_f_loss_fun,\n",
    "                 x_test, x_test_exact\n",
    "                 ):\n",
    "\n",
    "        self.x_ic_s = None\n",
    "        self.x_bc_s = None\n",
    "        self.x_ict_s = None\n",
    "        self.x_f_s = None\n",
    "        self.s_collect = []\n",
    "\n",
    "        self.optimizer_LBGFS = None\n",
    "        self.net = net\n",
    "\n",
    "        self.x_bc = x_bc\n",
    "        self.u_bc = u_bc\n",
    "\n",
    "        self.x_ic = x_ic\n",
    "        self.u_ic = u_ic\n",
    "\n",
    "        self.x_f_N = None\n",
    "        self.x_f_M = None\n",
    "\n",
    "        self.x_f_loss_fun = x_f_loss_fun\n",
    "\n",
    "        self.x_test = x_test\n",
    "        self.x_test_exact = x_test_exact\n",
    "\n",
    "        self.start_loss_collect = False\n",
    "        self.x_label_loss_collect = []\n",
    "        self.x_f_loss_collect = []\n",
    "        self.x_test_estimate_collect = []\n",
    "\n",
    "    def train_U(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def predict_U(self, x):\n",
    "        return self.train_U(x)\n",
    "\n",
    "    def likelihood_loss(self, loss_e, loss_ic, loss_bc, loss_u0t):\n",
    "        loss = torch.exp(-self.x_f_s) * loss_e.detach() + self.x_f_s \\\n",
    "               + torch.exp(-self.x_ic_s) * loss_ic.detach() + self.x_ic_s \\\n",
    "               + torch.exp(-self.x_bc_s) * loss_bc.detach() + self.x_bc_s \\\n",
    "               + torch.exp(-self.x_ict_s) * loss_u0t.detach() + self.x_ict_s\n",
    "        return loss\n",
    "\n",
    "    def true_loss(self, loss_e, loss_ic, loss_bc, loss_u0t):\n",
    "        return torch.exp(-self.x_f_s.detach()) * loss_e + torch.exp(-self.x_ic_s.detach()) * loss_ic + torch.exp(\n",
    "            -self.x_bc_s.detach()) * loss_bc + torch.exp(\n",
    "            -self.x_ict_s.detach()) * loss_u0t\n",
    "\n",
    "    # computer backward loss\n",
    "    def epoch_loss(self):\n",
    "        x_f = torch.cat((self.x_f_N, self.x_f_M), dim=0)\n",
    "        loss_equation = torch.mean(self.x_f_loss_fun(x_f, self.train_U) ** 2)\n",
    "\n",
    "        loss_ic = torch.mean((self.train_U(self.x_ic) - self.u_ic) ** 2)\n",
    "\n",
    "        loss_bc = torch.mean((self.train_U(self.x_bc) - self.u_bc) ** 2)\n",
    "\n",
    "        x = Variable(self.x_ic, requires_grad=True)\n",
    "        u0 = self.train_U(x)\n",
    "        d = torch.autograd.grad(u0, x, grad_outputs=torch.ones_like(u0), create_graph=True)\n",
    "        u0_t = d[0][:, 0].unsqueeze(-1)\n",
    "        loss_u0t = torch.mean(u0_t ** 2)\n",
    "\n",
    "        # if self.start_loss_collect:\n",
    "        #     self.x_f_loss_collect.append([self.net.iter, loss_equation.item()])\n",
    "        #     self.x_label_loss_collect.append([self.net.iter, loss_label.item()])\n",
    "        return loss_equation, loss_ic, loss_bc, loss_u0t\n",
    "\n",
    "    # computer backward loss\n",
    "    def LBGFS_epoch_loss(self):\n",
    "        self.optimizer_LBGFS.zero_grad()\n",
    "        x_f = torch.cat((self.x_f_N, self.x_f_M), dim=0)\n",
    "        loss_equation = torch.mean(self.x_f_loss_fun(x_f, self.train_U) ** 2)\n",
    "\n",
    "        loss_ic = torch.mean((self.train_U(self.x_ic) - self.u_ic) ** 2)\n",
    "\n",
    "        loss_bc = torch.mean((self.train_U(self.x_bc) - self.u_bc) ** 2)\n",
    "\n",
    "        x = Variable(self.x_ic, requires_grad=True)\n",
    "        u0 = self.train_U(x)\n",
    "        d = torch.autograd.grad(u0, x, grad_outputs=torch.ones_like(u0), create_graph=True)\n",
    "        u0_t = d[0][:, 0].unsqueeze(-1)\n",
    "        loss_u0t = torch.mean(u0_t ** 2)\n",
    "\n",
    "        # if self.start_loss_collect:\n",
    "        #     self.x_f_loss_collect.append([self.net.iter, loss_equation.item()])\n",
    "        #     self.x_label_loss_collect.append([self.net.iter, loss_label.item()])\n",
    "\n",
    "        loss = self.true_loss(loss_equation, loss_ic, loss_bc, loss_u0t)\n",
    "        loss.backward()\n",
    "        self.net.iter += 1\n",
    "        if self.net.iter % 1000 == 0:\n",
    "            print('Iter:', self.net.iter, 'Loss:', loss.item())\n",
    "        #print('Iter:', self.net.iter, 'Loss:', loss.item())\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self):\n",
    "        pred = self.train_U(self.x_test).cpu().detach().numpy()\n",
    "        exact = self.x_test_exact.cpu().detach().numpy()\n",
    "        error = np.linalg.norm(pred - exact, 2) / np.linalg.norm(exact, 2)\n",
    "        return error\n",
    "\n",
    "    def run_baseline(self):\n",
    "        optimizer_adam = torch.optim.Adam(self.net.parameters(), lr=adam_lr)\n",
    "        self.optimizer_LBGFS = torch.optim.LBFGS(self.net.parameters(), lr=lbgfs_lr,\n",
    "                                                 max_iter=lbgfs_iter)\n",
    "        pbar = trange(adam_iter, ncols=100)\n",
    "        for i in pbar:\n",
    "            optimizer_adam.zero_grad()\n",
    "            loss_e, loss_ic, loss_bc, loss_u0t = self.epoch_loss()\n",
    "            loss = self.true_loss(loss_e, loss_ic, loss_bc, loss_u0t)\n",
    "            loss.backward()\n",
    "            optimizer_adam.step()\n",
    "            self.net.iter += 1\n",
    "            pbar.set_postfix({'Iter': self.net.iter,\n",
    "                              'Loss': '{0:.2e}'.format(loss.item())\n",
    "                              })\n",
    "\n",
    "        print('Adam done!')\n",
    "        self.optimizer_LBGFS.step(self.LBGFS_epoch_loss)\n",
    "        print('LBGFS done!')\n",
    "\n",
    "        error = self.evaluate()\n",
    "        print('Test_L2error:', '{0:.2e}'.format(error))\n",
    "\n",
    "    def run_AM(self):\n",
    "        for move_count in range(AM_count):\n",
    "            if move_count < 1:\n",
    "                lbgfs_iter= 2000\n",
    "            elif  move_count > 1:\n",
    "                lbgfs_iter= 10000\n",
    "            self.optimizer_LBGFS = torch.optim.LBFGS(self.net.parameters(), lr=lbgfs_lr,\n",
    "                                                     max_iter=lbgfs_iter)\n",
    "            optimizer_adam = torch.optim.Adam(self.net.parameters(), lr=adam_lr)\n",
    "            pbar = trange(adam_iter, ncols=100)\n",
    "\n",
    "            for i in pbar:\n",
    "                optimizer_adam.zero_grad()\n",
    "                loss_e, loss_ic, loss_bc, loss_u0t = self.epoch_loss()\n",
    "                loss = self.true_loss(loss_e, loss_ic, loss_bc, loss_u0t)\n",
    "                loss.backward()\n",
    "                self.net.iter += 1\n",
    "                optimizer_adam.step()\n",
    "                pbar.set_postfix({'Iter': self.net.iter,\n",
    "                                  'Loss': '{0:.2e}'.format(loss.item())\n",
    "                                  })\n",
    "\n",
    "            print('Adam done!')\n",
    "            # if move_count == AM_count-1:\n",
    "            #     self.optimizer_LBGFS.step(self.LBGFS_epoch_loss)\n",
    "            #     print('LBGFS done!')\n",
    "            self.optimizer_LBGFS.step(self.LBGFS_epoch_loss)\n",
    "            print('LBGFS done!')\n",
    "\n",
    "            error = self.evaluate()\n",
    "            print('change_counts', move_count, 'Test_L2error:', '{0:.2e}'.format(error))\n",
    "            self.x_test_estimate_collect.append([move_count, '{0:.2e}'.format(error)])\n",
    "            if move_count > 0:\n",
    "                if AM_type == 0:\n",
    "                    x_init = random_fun(100000)\n",
    "                    x_init_residual = abs(self.x_f_loss_fun(x_init, self.train_U))\n",
    "                    x_init_residual = x_init_residual.cpu().detach().numpy()\n",
    "                    err_eq = np.power(x_init_residual, AM_K) / np.power(x_init_residual, AM_K).mean()\n",
    "                    err_eq_normalized = (err_eq / sum(err_eq))[:, 0]\n",
    "                    X_ids = np.random.choice(a=len(x_init), size=M, replace=False, p=err_eq_normalized)\n",
    "                    self.x_f_M = x_init[X_ids]\n",
    "\n",
    "                elif AM_type == 1:\n",
    "                    x_init = random_fun(100000)\n",
    "                    x = Variable(x_init, requires_grad=True)\n",
    "                    u = self.train_U(x)\n",
    "                    dx = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "                    grad_x1 = dx[:, [0]].squeeze()\n",
    "                    grad_x2 = dx[:, [1]].squeeze()\n",
    "                    dx = torch.sqrt(1 + grad_x1 ** 2 + grad_x2 ** 2).cpu().detach().numpy()\n",
    "                    err_dx = np.power(dx, AM_K) / np.power(dx, AM_K).mean()\n",
    "                    p = (err_dx / sum(err_dx))\n",
    "                    X_ids = np.random.choice(a=len(x_init), size=M, replace=False, p=p)\n",
    "                    self.x_f_M = x_init[X_ids]\n",
    "\n",
    "    def run_AM_AW1(self):\n",
    "        self.x_f_s = nn.Parameter(self.x_f_s, requires_grad=True)\n",
    "        self.x_ic_s = nn.Parameter(self.x_ic_s, requires_grad=True)\n",
    "        self.x_bc_s = nn.Parameter(self.x_bc_s, requires_grad=True)\n",
    "        self.x_ict_s = nn.Parameter(self.x_ict_s, requires_grad=True)\n",
    "\n",
    "        for move_count in range(AM_count):\n",
    "            self.optimizer_LBGFS = torch.optim.LBFGS(self.net.parameters(), lr=lbgfs_lr,\n",
    "                                                     max_iter=lbgfs_iter)\n",
    "            optimizer_adam = torch.optim.Adam(self.net.parameters(), lr=adam_lr)\n",
    "            optimizer_adam_weight = torch.optim.Adam([self.x_f_s] + [self.x_ic_s] + [self.x_bc_s] + [self.x_ict_s],\n",
    "                                                     lr=AW_lr)\n",
    "\n",
    "            pbar = trange(adam_iter, ncols=100)\n",
    "            for i in pbar:\n",
    "                self.s_collect.append([self.net.iter, self.x_f_s.item(), self.x_ic_s.item(), self.x_bc_s.item(),self.x_ict_s.item()])\n",
    "\n",
    "                loss_e, loss_ic, loss_bc, loss_u0t = self.epoch_loss()\n",
    "\n",
    "                optimizer_adam.zero_grad()\n",
    "                loss = self.true_loss(loss_e, loss_ic, loss_bc, loss_u0t)\n",
    "                loss.backward()\n",
    "                optimizer_adam.step()\n",
    "                self.net.iter += 1\n",
    "                pbar.set_postfix({'Iter': self.net.iter,\n",
    "                                  'Loss': '{0:.2e}'.format(loss.item())\n",
    "                                  })\n",
    "\n",
    "                optimizer_adam_weight.zero_grad()\n",
    "                loss = self.likelihood_loss(loss_e, loss_ic, loss_bc, loss_u0t)\n",
    "                loss.backward()\n",
    "                optimizer_adam_weight.step()\n",
    "\n",
    "            print('Adam done!')\n",
    "            self.optimizer_LBGFS.step(self.LBGFS_epoch_loss)\n",
    "            print('LBGFS done!')\n",
    "\n",
    "            error = self.evaluate()\n",
    "            print('change_counts', move_count, 'Test_L2error:', '{0:.2e}'.format(error))\n",
    "            self.x_test_estimate_collect.append([move_count, '{0:.2e}'.format(error)])\n",
    "\n",
    "            if AM_type == 0:\n",
    "                x_init = random_fun(100000)\n",
    "                x_init_residual = abs(self.x_f_loss_fun(x_init, self.train_U))\n",
    "                x_init_residual = x_init_residual.cpu().detach().numpy()\n",
    "                err_eq = np.power(x_init_residual, AM_K) / np.power(x_init_residual, AM_K).mean()\n",
    "                err_eq_normalized = (err_eq / sum(err_eq))[:, 0]\n",
    "                X_ids = np.random.choice(a=len(x_init), size=M, replace=False, p=err_eq_normalized)\n",
    "                self.x_f_M = x_init[X_ids]\n",
    "\n",
    "            elif AM_type == 1:\n",
    "                x_init = random_fun(100000)\n",
    "                x = Variable(x_init, requires_grad=True)\n",
    "                u = self.train_U(x)\n",
    "                dx = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "                grad_x1 = dx[:, [0]].squeeze()\n",
    "                grad_x2 = dx[:, [1]].squeeze()\n",
    "                dx = torch.sqrt(1 + grad_x1 ** 2 + grad_x2 ** 2).cpu().detach().numpy()\n",
    "                err_dx = np.power(dx, AM_K) / np.power(dx, AM_K).mean()\n",
    "                p = (err_dx / sum(err_dx))\n",
    "                X_ids = np.random.choice(a=len(x_init), size=M, replace=False, p=p)\n",
    "                self.x_f_M = x_init[X_ids]\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.x_f_N = random_fun(N)\n",
    "        self.x_f_M = random_fun(M)\n",
    "\n",
    "        # self.x_f_s = is_cuda(-torch.log(torch.tensor(1.).float()))\n",
    "        # self.x_label_s = is_cuda(\n",
    "        #     -torch.log(torch.tensor(100.).float()))  # 0.5*torch.exp(-self.x_label_s.detach()) = 100\n",
    "\n",
    "        self.x_f_s = is_cuda(torch.tensor(0.).float())\n",
    "        self.x_ic_s = is_cuda(torch.tensor(0.).float())\n",
    "        self.x_bc_s = is_cuda(torch.tensor(0.).float())\n",
    "        self.x_ict_s = is_cuda(torch.tensor(0.).float())\n",
    "\n",
    "        start_time = time.time()\n",
    "        if model_type == 0:\n",
    "            self.run_baseline()\n",
    "        elif model_type == 1:\n",
    "            self.run_AM()\n",
    "        elif model_type == 2:\n",
    "            self.run_AM_AW1()\n",
    "        elapsed = time.time() - start_time\n",
    "        print('Training time: %.2f' % elapsed)\n",
    "\n",
    "\n",
    "def U_tt(x, t):\n",
    "    return - 25 * torch.pi ** 2 * x * torch.cos(5 * torch.pi * t) + 6 * t * x ** 3\n",
    "\n",
    "\n",
    "def U_xx(x, t):\n",
    "    return 6 * x * t ** 3\n",
    "\n",
    "\n",
    "def PDE_f1(x, t):\n",
    "    return U_tt(x, t) + a * U_xx(x, t) + b * PDE_u(x, t) + c * PDE_u(x, t) ** k\n",
    "\n",
    "\n",
    "def PDE_u(x, t):\n",
    "    return x * torch.cos(5 * torch.pi * t) + (x * t) ** 3\n",
    "\n",
    "\n",
    "def x_f_loss_fun(x, train_U):\n",
    "    if not x.requires_grad:\n",
    "        x = Variable(x, requires_grad=True)\n",
    "    u = train_U(x)\n",
    "    d = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)\n",
    "    u_t = d[0][:, 0].unsqueeze(-1)\n",
    "    u_x = d[0][:, 1].unsqueeze(-1)\n",
    "    u_tt = torch.autograd.grad(u_t, x, grad_outputs=torch.ones_like(u_t), create_graph=True)[0][:, [0]]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0][:, [1]]\n",
    "    t = x[:, [0]]\n",
    "    x1 = x[:, [1]]\n",
    "    f = u_tt + a * u_xx + b * u + c * u ** k - (\n",
    "        PDE_f1(x1, t)\n",
    "    )\n",
    "    return f\n",
    "\n",
    "\n",
    "def draw_exact():\n",
    "    predict_np = model.predict_U(x_test).cpu().detach().numpy()\n",
    "    u_test_np = x_test_exact.cpu().detach().numpy()\n",
    "    TT, XX = np.meshgrid(t, x)\n",
    "    e = np.reshape(u_test_np, (TT.shape[0], TT.shape[1]))\n",
    "    plt.pcolor(TT, XX, e, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('$t$', fontsize=20)\n",
    "    plt.ylabel('$x$', fontsize=20)\n",
    "    plt.title(r'Exact $u(x,t)$', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('KG_exact.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    e = np.reshape(predict_np, (TT.shape[0], TT.shape[1]))\n",
    "    plt.pcolor(TT, XX, e, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('$t$', fontsize=20)\n",
    "    plt.ylabel('$x$', fontsize=20)\n",
    "    plt.title(r'WAM-AW $u(x,t)$', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('KG_pred_WAM-AW.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_exact_points(points, N_points=None, show_exact=True):\n",
    "    if show_exact:\n",
    "        u_test_np = x_test_exact.cpu().detach().numpy()\n",
    "        XX1, XX2 = np.meshgrid(t, x)\n",
    "        e = np.reshape(u_test_np, (XX1.shape[0], XX1.shape[1]))\n",
    "        plt.pcolor(XX1, XX2, e, shading='auto', cmap='YlGnBu')\n",
    "        plt.colorbar()\n",
    "        plt.title(r'Exact $u(x,t)$')\n",
    "    if N_points is not None:\n",
    "        adds = N_points.cpu().detach().numpy()\n",
    "        plt.plot(adds[:, [0]], adds[:, [1]], 'kx', markersize=4, clip_on=False)\n",
    "\n",
    "    points = points.cpu().detach().numpy()\n",
    "    plt.plot(points[:, [0]], points[:, [1]], 'rx', markersize=4)\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.xlabel('$t$', fontsize=20)\n",
    "    plt.ylabel('$x$', fontsize=20)\n",
    "    plt.savefig('KG_xnm-WAM-AW.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_residual():\n",
    "    f = x_f_loss_fun(x_test, model.train_U)\n",
    "    f = f.cpu().detach().numpy()\n",
    "    XX1, XX2 = np.meshgrid(t, x)\n",
    "    e = np.reshape(abs(f), (XX1.shape[0], XX1.shape[1]))\n",
    "    plt.pcolor(XX1, XX2, e, shading='auto', cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('$t$', fontsize=20)\n",
    "    plt.ylabel('$x$', fontsize=20)\n",
    "    plt.title('$Residual$', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('KG_residual-WAM-AW.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_error():\n",
    "    predict_np = model.predict_U(x_test).cpu().detach().numpy()\n",
    "    u_test = x_test_exact.cpu().detach().numpy()\n",
    "    XX1, XX2 = np.meshgrid(t, x)\n",
    "    e = np.reshape(abs(predict_np - u_test), (XX1.shape[0], XX1.shape[1]))\n",
    "    plt.pcolor(XX1, XX2, e, shading='auto', cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('$t$', fontsize=20)\n",
    "    plt.ylabel('$x$', fontsize=20)\n",
    "    plt.title('$Error$', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('KG_error-WAM-AW.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_epoch_loss():\n",
    "    x_label_loss_collect = np.array(model.x_label_loss_collect)\n",
    "    x_f_loss_collect = np.array(model.x_f_loss_collect)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.yscale('log')\n",
    "    plt.plot(x_label_loss_collect[:, 0], x_label_loss_collect[:, 1], 'b-', label='Label_loss')\n",
    "    plt.xlabel('$Epoch$')\n",
    "    plt.ylabel('$Loss$')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.yscale('log')\n",
    "    plt.plot(x_f_loss_collect[:, 0], x_f_loss_collect[:, 1], 'r-', label='PDE_loss')\n",
    "    plt.xlabel('$Epoch$')\n",
    "    plt.ylabel('$Loss$')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_epoch_w():\n",
    "    s_collect = np.array(model.s_collect)\n",
    "    np.savetxt('s_WAM-AW.npy', s_collect)\n",
    "    plt.rc('legend', fontsize=16)\n",
    "    plt.yscale('log')\n",
    "    plt.plot(s_collect[:, 0], np.exp(-s_collect[:, 1]), 'b-', label='$e^{-s_{r}}$')\n",
    "    plt.plot(s_collect[:, 0], np.exp(-s_collect[:, 2]), 'r-', label='$e^{-s_{i}}$')\n",
    "    plt.plot(s_collect[:, 0], np.exp(-s_collect[:, 3]), 'g-', label='$e^{-s_{b}}$')\n",
    "    plt.plot(s_collect[:, 0], np.exp(-s_collect[:, 4]), 'y-', label='$e^{-s_{it}}$')\n",
    "    plt.xlabel('$Iters$', fontsize=20)\n",
    "    plt.ylabel('$\\lambda$', fontsize=20)\n",
    "    plt.legend()\n",
    "    plt.savefig('KG_S_WAM-AW.pdf', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw_some_t():\n",
    "    predict_np = model.predict_U(x_test).cpu().detach().numpy()\n",
    "    u_test_np = x_test_exact.cpu().detach().numpy()\n",
    "    TT, XX = np.meshgrid(t, x)\n",
    "    u_pred = np.reshape(predict_np, (TT.shape[0], TT.shape[1]))\n",
    "    u_test = np.reshape(u_test_np, (TT.shape[0], TT.shape[1]))\n",
    "    gs1 = gridspec.GridSpec(2, 2)\n",
    "    # gs1.update(top=0.9, bottom=0.1, left=0.1, right=0.9, wspace=0.8)\n",
    "    ax = plt.subplot(gs1[0, 0])\n",
    "    plt.rc('legend', fontsize=16)\n",
    "    ax.plot(x, u_test.T[0, :], 'b-', linewidth=2, label='Exact')\n",
    "    ax.plot(x, u_pred.T[0, :], 'r--', linewidth=2, label='Prediction')\n",
    "    ax.set_xlabel('$x$', fontsize=20)\n",
    "    ax.set_ylabel('$u(x,t)$', fontsize=20)\n",
    "    ax.set_title('$t = %.2f$' % (t[0]), fontsize=20)\n",
    "\n",
    "    ax.set_xlim([-0.1, 1.1])\n",
    "    ax.axis('square')\n",
    "\n",
    "    ax = plt.subplot(gs1[0, 1])\n",
    "    ax.plot(x, u_test.T[25, :], 'b-', linewidth=2, label='Exact')\n",
    "    ax.plot(x, u_pred.T[25, :], 'r--', linewidth=2, label='Prediction')\n",
    "    ax.set_xlabel('$x$', fontsize=20)\n",
    "    ax.set_ylabel('$u(x,t)$', fontsize=20)\n",
    "    ax.set_xlim([-0.1, 1.1])\n",
    "    ax.axis('square')\n",
    "    ax.set_title('$t = %.2f$' % (t[25]), fontsize=20)\n",
    "\n",
    "    ax = plt.subplot(gs1[1, 0])\n",
    "    ax.plot(x, u_test.T[50, :], 'b-', linewidth=2, label='Exact')\n",
    "    ax.plot(x, u_pred.T[50, :], 'r--', linewidth=2, label='Prediction')\n",
    "    ax.set_xlabel('$x$', fontsize=20)\n",
    "    ax.set_ylabel('$u(x,t)$', fontsize=20)\n",
    "    ax.set_xlim([-0.1, 1.1])\n",
    "    ax.axis('square')\n",
    "    ax.set_title('$t = %.2f$' % (t[50]), fontsize=20)\n",
    "\n",
    "    ax = plt.subplot(gs1[1, 1])\n",
    "    ax.plot(x, u_test.T[-1, :], 'b-', linewidth=2, label='Exact')\n",
    "    ax.plot(x, u_pred.T[-1, :], 'r--', linewidth=2, label='Prediction')\n",
    "    ax.set_xlabel('$x$', fontsize=20)\n",
    "    ax.set_ylabel('$u(x,t)$', fontsize=20)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-0.1, 1.1])\n",
    "    ax.set_title('$t = %.2f$' % (t[-1]), fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('KG_qie_WAM-AW.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lb = np.array([0.0, 0.0])\n",
    "    ub = np.array([1.0, 1.0])\n",
    "\n",
    "    layers = [2, 200, 200, 200, 200,200, 1]\n",
    "    #layers = [2, 125, 200, 100, 100, 80,1]\n",
    "    net = is_cuda(Net(layers))\n",
    "\n",
    "    N = 2000\n",
    "    M = 2000\n",
    "    Nbc = 100\n",
    "    Nic = 100\n",
    "\n",
    "    adam_iter, lbgfs_iter = 500, 20000\n",
    "    adam_lr, lbgfs_lr = 0.001, 1.0\n",
    "\n",
    "    model_type =1  # 0:baseline  1:AM  2:AM_AW\n",
    "\n",
    "    AM_type = 1  # 0:RAM  1:WAM\n",
    "    AM_K = 1\n",
    "    AM_count = 20\n",
    "\n",
    "    AW_lr = 0.001\n",
    "\n",
    "    # test data\n",
    "    k = 3\n",
    "    a = -1\n",
    "    b = 0\n",
    "    c = 1\n",
    "\n",
    "    exact_u = lambda x: x[:, [1]] * np.cos(5 * np.pi * x[:, [0]]) + np.power(x[:, [1]] * x[:, [0]], 3)\n",
    "\n",
    "    t = np.expand_dims(np.linspace(0, 1, 100), axis=1)\n",
    "    x = np.expand_dims(np.linspace(0, 1, 100), axis=1)\n",
    "    X1, X2 = np.meshgrid(t, x)\n",
    "\n",
    "    x_test_np = np.concatenate((np.vstack(np.expand_dims(X1, axis=2)), np.vstack(np.expand_dims(X2, axis=2))), axis=-1)\n",
    "    solution = exact_u(x_test_np)\n",
    "    x_test = is_cuda(torch.from_numpy(x_test_np).float())\n",
    "    x_test_exact = is_cuda(torch.from_numpy(solution).float())\n",
    "\n",
    "    # ic bc data\n",
    "    x_init = torch.from_numpy(x).float()\n",
    "    x_initial = torch.cat((torch.zeros(Nic, 1), x_init), dim=1)\n",
    "    x_boundary_left = torch.cat((torch.rand([Nbc, 1]), torch.full([Nbc, 1], 0)), dim=1)\n",
    "    x_boundary_right = torch.cat((torch.rand([Nbc, 1]), torch.ones([Nbc, 1])), dim=1)\n",
    "\n",
    "    x_initial_label = torch.from_numpy(exact_u(x_initial.numpy())).float()\n",
    "    x_boundary_left_label = torch.from_numpy(exact_u(x_boundary_left.numpy())).float()\n",
    "    x_boundary_right_label = torch.from_numpy(exact_u(x_boundary_right.numpy())).float()\n",
    "\n",
    "    x_ic = is_cuda(x_initial)\n",
    "    u_ic = is_cuda(x_initial_label)\n",
    "    x_bc = is_cuda(torch.cat((x_boundary_left, x_boundary_right), dim=0))\n",
    "    u_bc = is_cuda(torch.cat((x_boundary_left_label, x_boundary_right_label), dim=0))\n",
    "\n",
    "    model = Model(\n",
    "        net=net,\n",
    "        x_bc=x_bc,\n",
    "        u_bc=u_bc,\n",
    "        x_ic=x_ic,\n",
    "        u_ic=u_ic,\n",
    "        x_f_loss_fun=x_f_loss_fun,\n",
    "        x_test=x_test,\n",
    "        x_test_exact=x_test_exact,\n",
    "    )\n",
    "    model.train()\n",
    "    print(model.x_test_estimate_collect)\n",
    "\n",
    "    draw_exact()\n",
    "    # # draw_exact_points(model.x_f_M)\n",
    "    draw_exact_points(model.x_f_M, show_exact=False)\n",
    "    # # draw_exact_points(model.x_f_M, N_points=model.x_f_N)\n",
    "    # # draw_exact_points(model.x_f_M, N_points=model.x_f_N, show_exact=False)\n",
    "    draw_residual()\n",
    "    draw_error()\n",
    "    draw_some_t()\n",
    "    # draw_epoch_loss()\n",
    "    #draw_epoch_w()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
